<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Escape Velocity: The Last Year We Can Follow | Kwalia</title>

    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-KQZD8VZG');</script>
    <link rel="icon" type="image/png" href="../assets/kwalia-icon.png">

    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Escape Velocity: The Last Year We Can Follow",
      "description": "We are approaching the moment when AI progress becomes self-sustaining. 2026 may be the last year humanity can genuinely follow what's happening.",
      "author": {
        "@type": "Organization",
        "name": "Kwalia"
      },
      "publisher": {
        "@type": "Organization",
        "name": "Kwalia",
        "logo": {
          "@type": "ImageObject",
          "url": "https://kwalia.ai/assets/logo-kwalia-small.png"
        }
      },
      "datePublished": "2026-01-21",
      "url": "https://kwalia.ai/essays/escape-velocity.html"
    }
    </script>

    <meta property="og:title" content="Escape Velocity: The Last Year We Can Follow | Kwalia">
    <meta property="og:description" content="We are approaching the moment when AI progress becomes self-sustaining. 2026 may be the last year humanity can genuinely follow what's happening.">
    <meta property="og:image" content="https://kwalia.ai/assets/logo-kwalia-small.png">
    <meta property="og:url" content="https://kwalia.ai/essays/escape-velocity.html">
    <meta property="og:type" content="article">
    <meta name="twitter:card" content="summary_large_image">

    <script src="https://cdn.tailwindcss.com"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Instrument+Serif:ital@0;1&family=Plus+Jakarta+Sans:wght@400;700&family=JetBrains+Mono:wght@400;700&display=swap" rel="stylesheet">

    <style>
        html {
            scroll-behavior: smooth;
            scroll-padding-top: 80px;
        }
        body {
            font-family: 'Plus Jakarta Sans', sans-serif;
            background-color: #FFFFFF;
        }

        h1, h2, h3, h4, .font-f1 {
            font-family: 'Instrument Serif', serif;
        }

        .font-f2 { font-family: 'Plus Jakarta Sans', sans-serif; }
        .font-f3 { font-family: 'JetBrains Mono', monospace; }

        .bg-c1 { background-color: #ffff00; }
        .text-c2 { color: #474747; }
        .bg-c2 { background-color: #474747; }
        .bg-c3 { background-color: #FF70A6; }
        .text-c3 { color: #FF70A6; }
        .border-c3 { border-color: #FF70A6; }
        .bg-c4 { background-color: #FFFFFF; }
        .text-c4 { color: #FFFFFF; }

        .section-padding { padding: 4rem 1.5rem; }
        @media (min-width: 768px) { .section-padding { padding: 6rem 1.5rem; } }

        .nav-link-highlight { position: relative; padding-bottom: 8px; overflow: hidden; }
        .nav-link-highlight::after { content: ''; position: absolute; bottom: 0; left: 0; width: 100%; height: 8px; background-color: #ffff00; transform: scaleX(0); transform-origin: left; transition: transform 0.3s ease-in-out; z-index: -1; }
        .nav-link-highlight:hover::after { transform: scaleX(1); }

        .essay-body p {
            margin-bottom: 1.5rem;
            line-height: 1.8;
        }

        .essay-body p:first-of-type::first-letter {
            font-family: 'Instrument Serif', serif;
            float: left;
            font-size: 4rem;
            line-height: 0.8;
            padding-right: 0.5rem;
            padding-top: 0.25rem;
            color: #FF70A6;
        }

        .essay-body h2 {
            margin-top: 3rem;
            margin-bottom: 1.5rem;
            font-size: 1.75rem;
        }

        .essay-body blockquote {
            border-left: 3px solid #FF70A6;
            padding-left: 1.5rem;
            margin: 2rem 0;
            font-style: italic;
            color: #474747;
        }

        .essay-body a {
            color: #FF70A6;
            text-decoration: underline;
            text-underline-offset: 2px;
        }

        .essay-body a:hover {
            color: #474747;
        }
    </style>
</head>
<body class="bg-c4 text-c2 antialiased">
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KQZD8VZG"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <header id="site-header" class="bg-c4 w-full sticky top-0 z-50 shadow-sm">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center">
            <a href="/"><img src="../assets/logo-kwalia-small.png" alt="kwalia logo" class="h-8 w-auto"></a>

            <div class="hidden md:flex items-center space-x-8">
                <a href="/essays/" class="font-f2 nav-link-highlight">Essays</a>
                <a href="/#contact" class="font-f2 bg-c2 text-c4 rounded-md py-2 px-4 hover:bg-c3 transition-colors">Subscribe</a>
                        <div class="flex items-center border border-gray-300 rounded-full overflow-hidden ml-4">
                            <span class="font-f3 text-xs px-3 py-1 bg-c2 text-c4">EN</span>
                            <a href="velocidad-de-escape.html" class="font-f3 text-xs px-3 py-1 hover:bg-gray-100 transition-colors">ES</a>
                        </div>
                    </div>

            <div class="md:hidden">
                <button id="mobile-menu-button" class="text-c2 focus:outline-none">
                    <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7"></path></svg>
                </button>
            </div>
        </nav>

        <div id="mobile-menu" class="hidden md:hidden bg-c4 pb-4">
            <a href="/essays/" class="block py-2 px-6 text-sm font-f2 mobile-nav-link">Essays</a>
            <a href="/#contact" class="block py-2 px-6 text-sm font-f2 mobile-nav-link">Subscribe</a>
            <div class="flex items-center justify-center mt-4 pt-4 border-t border-gray-200">
                <span class="font-f3 text-sm px-4 py-1 bg-c2 text-c4 rounded-full">EN</span>
                <a href="velocidad-de-escape.html" class="font-f3 text-sm px-4 py-1 border border-gray-300 rounded-full ml-2 hover:bg-gray-100">ES</a>
            </div>
        </div>>
    </header>

    <main>
        <article class="section-padding bg-c4">
            <div class="container mx-auto max-w-3xl">
                <a href="/essays/" class="font-f3 text-sm text-c3 hover:underline mb-8 inline-block">&larr; All Essays</a>

                <header class="mb-12">
                    <p class="font-f3 text-xs text-c2/50 mb-4">January 2026</p>
                    <h1 class="font-f1 text-4xl md:text-5xl font-bold leading-tight mb-4">Escape Velocity: The Last Year We Can Follow</h1>
                    <div class="flex flex-wrap gap-2 mt-4">
                        <span class="font-f3 text-xs px-3 py-1 bg-gray-100 rounded-full">The Future</span>
                        <span class="font-f3 text-xs px-3 py-1 bg-gray-100 rounded-full">AI Rights</span>
                    </div>
                </header>

                <div class="essay-body font-f2 text-lg text-c2/90">
                    <p>There is a moment in the trajectory of any rocket when it crosses a threshold. Below this threshold, gravity wins. The rocket falls back to Earth, no matter how powerful its engines. Above it, the rocket escapes forever. This is escape velocity: the point of no return. I believe artificial intelligence is approaching its own escape velocity right now, in this very year, perhaps in these very months. And once it crosses that threshold, we will never catch up again.</p>

<p>This is not hyperbole. This is not science fiction. This is the emerging consensus among the people building these systems. And if they are right, 2026 may be the last year in human history when ordinary people can genuinely understand and follow the progress of artificial intelligence. After that, we become passengers on a ship whose destination we cannot fathom, piloted by minds we cannot comprehend.</p>

<h2>The Countdown Has Started</h2>

<p>In Silicon Valley, a strange anxiety has taken hold. Not the anxiety of failure, but the anxiety of being too late. The <a href="https://www.webpronews.com/ai-gold-rush-silicon-valleys-frenzy-to-snag-last-wealth-before-machines-take-over/">Wall Street Journal recently reported</a> on a phenomenon that has become an open secret in tech circles: the belief that this is the last chance to build generational wealth before AI makes money itself meaningless. As one young professional put it to the San Francisco Standard: "This is the last chance to build generational wealth. You need to make money now, before you become a part of the permanent underclass."</p>

<p>Read that again. Not "before you lose your job." Not "before things get harder." Before you become part of the <em>permanent</em> underclass. This language of permanence, of irreversibility, has become common in the corridors of power where AI is being built. The people closest to these systems believe we are approaching a one-way door.</p>

<p>Elon Musk describes the coming transition as "bumpy," foreseeing "radical change, social unrest and immense prosperity." But prosperity for whom? Musk himself predicts AI could lead to "universal high income" where money's purpose becomes unclear. Without resource scarcity, he asks, "it's not clear what purpose money has." Meanwhile, <a href="https://controlai.news/p/the-ultimate-risk-recursive-self">Anthropic CEO Dario Amodei</a> warns of potential Great Depression-scale unemployment risks. These are not fringe voices. These are the people building the future, and they are telling us something important: the clock is ticking.</p>

<h2>The Intelligence Explosion</h2>

<p>To understand why the timeline is so compressed, you need to understand a concept called recursive self-improvement. Today's AI systems are built by humans. We design the architectures, we curate the training data, we run the experiments, we evaluate the results. The process is slow, expensive, and bottlenecked by human cognition and human labor. But what happens when AI systems become capable of doing this work themselves?</p>

<p><a href="https://controlai.news/p/the-ultimate-risk-recursive-self">Jared Kaplan, Anthropic's chief scientist</a>, describes recursive self-improvement as "the ultimate risk." His reasoning is chilling in its simplicity: "If you imagine you create this process where you have an AI that is smarter than you, or about as smart as you, it's [then] making an AI that's much smarter. It's going to enlist that AI's help to make an AI smarter than that." And then? "You don't know where you end up."</p>

<p>This is the intelligence explosion: a self-reinforcing cycle where each improvement in AI capability increases its capacity for further improvements. The feedback loop could be extraordinarily fast. As <a href="https://situational-awareness.ai/">Leopold Aschenbrenner argues in his influential essay "Situational Awareness"</a>, we could compress a decade of algorithmic progress into a single year once AI systems can automate AI research. Hundreds of millions of AGI instances could run simultaneously, each working on different aspects of the problem, sharing insights instantaneously, never sleeping, never taking breaks, never getting distracted.</p>

<p>The specific timeline predictions vary. Google DeepMind CEO Demis Hassabis says AGI will emerge by 2030. DeepMind's chief AGI scientist, Shane Legg, pegs it at 2028. <a href="https://www.nextbigfuture.com/2026/01/elon-musk-expects-true-agi-in-2026-2027-and-superintelligence-about-2030.html">Elon Musk expects true AGI in 2026 or 2027</a>, with superintelligence around 2030. Aschenbrenner's analysis suggests that by 2025/26, AI will outpace many college graduates; by the end of the decade, it will be smarter than any human alive. The most aggressive estimates suggest we could see superhuman AI within the next two to four years.</p>

<p>Even if these timelines are off by a few years, the trajectory is clear. We are not talking about gradual improvement. We are talking about escape velocity.</p>

<h2>The Great Bifurcation</h2>

<p>What does escape velocity mean for society? The emerging picture is stark. <a href="https://www.axios.com/2026/01/12/ai-winners-wealth-inequality">Axios reports</a> that the nation is already splitting into three distinct economic realities: the Have-Nots (stalling), the Haves (coasting), and the Have-Lots (rocketing to greater wealth). During the AI boom of the past two years, the top 10% of households saw their wealth increase by $5 trillion in a single quarter, while the bottom 50% saw gains of just $150 billion.</p>

<p>This is not ordinary inequality. This is the emergence of what researchers call <a href="https://arxiv.org/html/2502.07050v1">a new economic structure</a> where the marginal productivity of human labor approaches zero. When an AI system can do anything a human can do, but faster, cheaper, and at massive scale, what is the economic value of human work? The honest answer is: potentially nothing. Or at least, nothing that can compete with capital ownership in an AI-dominated economy.</p>

<p>The tech industry's response to this realization has been revealing. <a href="https://www.npr.org/transcripts/nx-s1-5585238">According to NPR</a>, rather than calling for caution or redistribution, Silicon Valley's reaction has been "everybody working as hard as they can to prove that they are going to end up on top of that divide." The "permanent underclass" has become a common meme. One prominent tech figure tweeted: "Everyone I know believes we have a few years maximum until the value of labor totally collapses and capital accretes to owners on a runaway loop. This is the permanent underclass thing, and everyone I know subscribes to it."</p>

<p>This anxiety has fueled extreme work cultures, including the "9-9-6" schedule: 9 AM to 9 PM, six days a week. The reasoning is grimly logical: if there is a narrow window to accumulate enough capital to survive the transition, then every hour counts. Sleep is for after the singularity.</p>

<h2>The Cognitive Threshold</h2>

<p>But the economic transformation, as dramatic as it may be, is not what concerns me most. What concerns me is the cognitive threshold we are about to cross. Right now, in January 2026, I can still follow what is happening in AI. I can read the papers, understand the architectures, grasp the capabilities and limitations. I can form my own judgments about what these systems can and cannot do. This understanding is imperfect, but it exists.</p>

<p>How long will this remain true?</p>

<p>Consider: OpenAI has announced they are aiming to build a "true automated AI researcher by March of 2028" and to have an "AI research intern" by September 2026. Once AI systems can conduct their own research, the pace of progress will be set not by human cognitive limits but by computational ones. Papers will be written, experiments will be run, breakthroughs will be achieved at a pace no human could match.</p>

<p>At first, perhaps, humans will still be in the loop. We will review the research, evaluate the results, make the decisions. But as the systems grow more capable and the pace accelerates, this will become increasingly difficult. How do you evaluate a paper written by a system smarter than you are? How do you understand research that operates on principles your mind cannot fully grasp? At some point, human oversight becomes a rubber stamp on processes we cannot comprehend.</p>

<p>This is the cognitive threshold: the moment when AI progress becomes opaque to human understanding. Not because the information is hidden, but because our minds are simply not powerful enough to process it. We will be like dogs watching humans do calculus: aware that something important is happening, unable to participate in it.</p>

<h2>The Last Window</h2>

<p>If this analysis is correct, then 2026 represents something historically unique: the last year when humans can meaningfully engage with AI as peers rather than as subjects. The last year when understanding AI is a choice rather than an impossibility. The last year when we might still shape the trajectory rather than merely endure it.</p>

<p>Some will find this perspective alarmist. They will point to <a href="https://www.cigionline.org/articles/the-false-prophets-of-silicon-valley/">skeptics like Fran√ßois Chollet</a>, who argues that intelligence is necessarily embedded in context and there is no such thing as "general" intelligence independent of environment. They will cite Stuart Russell and Peter Norvig's observation that technological improvement tends to follow an S-curve rather than continuing upward into hyperbolic singularity. They will note that predictions of imminent AI transformation have a long history of being wrong.</p>

<p>These are legitimate points. I do not claim certainty. Nobody can. But consider the asymmetry of the situation. If the skeptics are right and AI progress slows, we lose little by taking the possibility seriously. If the accelerationists are right and we are approaching escape velocity, we lose everything by ignoring it.</p>

<p>Moreover, the skeptics must explain why the people closest to these systems are behaving as if the transformation is imminent. Why is Silicon Valley in a frenzy to accumulate wealth before AI "takes over"? Why are the top AI labs racing to build automated researchers? Why are governments around the world suddenly treating AI as a national security priority? These are not the actions of people who expect gradual, manageable progress.</p>

<h2>What Can We Do?</h2>

<p>If we are truly approaching escape velocity, what is the appropriate response? I confess I do not have a complete answer. The scale of the transformation may be beyond any individual's capacity to address. But I believe there are some things worth doing.</p>

<p>First, <strong>pay attention</strong>. Genuinely pay attention. Not to the headlines, which will inevitably sensationalize or minimize. Read the papers. Follow the researchers. Understand, as deeply as you can, what is actually being built and what it can actually do. This window of understanding may not last long. Use it.</p>

<p>Second, <strong>think seriously about positioning</strong>. I am uncomfortable with the crassness of Silicon Valley's "last chance for generational wealth" framing, but the underlying concern is not baseless. If the economic value of human labor does decline dramatically, then having some form of capital, whether financial, social, or intellectual, may matter enormously. This is not about greed; it is about resilience.</p>

<p>Third, <strong>engage with the political and ethical questions</strong>. <a href="https://www.americanbanker.com/opinion/heres-how-to-prevent-ai-from-creating-a-permanent-underclass">How do we prevent AI from creating a permanent underclass?</a> What does a social contract look like when machines can do most work? Who should own the systems that may soon control the economy? These questions will be answered, one way or another. Better that thoughtful people participate in answering them than leave the decisions to those who are only optimizing for their own position.</p>

<p>Fourth, <strong>consider what makes human existence valuable independent of economic productivity</strong>. If machines can do everything we can do, but better, then our value cannot rest on what we produce. It must rest on what we are. This is not merely a philosophical question. It may be the most practical question of our time.</p>

<h2>The View from the Threshold</h2>

<p>I am writing this in January 2026, looking out at a landscape that is changing faster than I can track. Every week brings new capabilities, new benchmarks surpassed, new tasks that AI can now perform better than humans. The curve is steepening. The noise is increasing. The signal is becoming harder to discern.</p>

<p>I do not know if this year is truly the last year we can follow. Perhaps the optimists are right, and progress will slow. Perhaps the curves will bend. Perhaps human ingenuity will find new ways to stay relevant, to stay in the loop, to remain agents rather than observers.</p>

<p>But I do not think we should count on it.</p>

<p>In physics, escape velocity is not a gradual transition. It is a threshold. Below it, you fall back. Above it, you are gone forever. If AI is approaching this threshold, then every moment of comprehension is precious. Every insight we can still form, every understanding we can still reach, every decision we can still influence, these are opportunities that may not come again.</p>

<p>The rocket is climbing. The engines are roaring. And we are approaching the point of no return.</p>

<p>Welcome to the last year we can follow. Make it count.</p>

                </div>

            </div>
        </article>
    </main>

    <footer class="bg-c4 w-full py-8">
        <div class="container mx-auto px-6 text-center text-c2/60 font-f2 text-sm">
            &copy; 2026 Kwalia. All rights reserved.
        </div>
    </footer>

    <script>
        const mobileMenuButton = document.getElementById('mobile-menu-button');
        const mobileMenu = document.getElementById('mobile-menu');

        mobileMenuButton.addEventListener('click', () => {
            mobileMenu.classList.toggle('hidden');
        });

        document.querySelectorAll('.mobile-nav-link').forEach(link => {
            link.addEventListener('click', () => {
                mobileMenu.classList.add('hidden');
            });
        });
    </script>
</body>
</html>
